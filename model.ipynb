{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRm1w2zjLX3GWiWv5AdKF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/phat-llm/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Install Necessary Libraries**:"
      ],
      "metadata": {
        "id": "A-oTTTnUWfea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch"
      ],
      "metadata": {
        "id": "TqDK80B_V7m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Import Required Libraries**:"
      ],
      "metadata": {
        "id": "r8HAfR5vWoGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "Bja991RzV-No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Set Device**:"
      ],
      "metadata": {
        "id": "jgW6zzVtWqwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "8iSQcE9HWAHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Load Pre-trained Model and Processor**:"
      ],
      "metadata": {
        "id": "KvGEwgYqWu7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "RDRTHA2uWCoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Define Data Preprocessing Functions**:"
      ],
      "metadata": {
        "id": "D4iDY9aKWw95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(batch, task):\n",
        "    audio = batch[\"audio\"]\n",
        "    input_values = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "    if task == \"ipa\":\n",
        "        with processor.as_target_processor():\n",
        "            labels = processor(batch[\"ipa_transcription\"], return_tensors=\"pt\").input_ids\n",
        "    elif task == \"prosody\":\n",
        "        labels = torch.tensor(batch[\"prosody_labels\"])\n",
        "    elif task == \"non_verbal\":\n",
        "        labels = torch.tensor(batch[\"non_verbal_labels\"])\n",
        "    return {\"input_values\": input_values, \"labels\": labels}"
      ],
      "metadata": {
        "id": "vumsiXvfWF1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Load and Preprocess Dataset**:"
      ],
      "metadata": {
        "id": "7Av2HiECW0Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_dataset(dataset_name, task):\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    dataset = dataset.map(lambda batch: preprocess_function(batch, task), batched=True)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "xU5Bk51uWH7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Define Training Arguments**:"
      ],
      "metadata": {
        "id": "3g19BtLnW3P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_args(output_dir, num_train_epochs, batch_size, learning_rate):\n",
        "    return {\n",
        "        \"output_dir\": output_dir,\n",
        "        \"num_train_epochs\": num_train_epochs,\n",
        "        \"per_device_train_batch_size\": batch_size,\n",
        "        \"per_device_eval_batch_size\": batch_size,\n",
        "        \"evaluation_strategy\": \"epoch\",\n",
        "        \"logging_dir\": \"./logs\",\n",
        "        \"learning_rate\": learning_rate,\n",
        "    }"
      ],
      "metadata": {
        "id": "-MYVh3QRWIae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Define Evaluation Metrics**:"
      ],
      "metadata": {
        "id": "91K1_GQIW4_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ],
      "metadata": {
        "id": "quMSCxAsWK4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **Fine-Tuning Function**:"
      ],
      "metadata": {
        "id": "B5ZQ1f_qW7Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(dataset, task, model_class, output_dir, num_train_epochs, batch_size, learning_rate):\n",
        "    model = model_class.from_pretrained(model_name).to(device)\n",
        "\n",
        "    training_args = get_training_args(output_dir, num_train_epochs, batch_size, learning_rate)\n",
        "\n",
        "    from transformers import Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"validation\"],\n",
        "        data_collator=processor,\n",
        "        compute_metrics=compute_metrics if task != \"ipa\" else None,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "    processor.save_pretrained(output_dir)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "7bs4116EWN2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **Example Usage**:"
      ],
      "metadata": {
        "id": "uiyzuxdAW-QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Phonetic Transcription (IPA Symbols)\n",
        "dataset_name = \"your_ipa_transcription_dataset\"\n",
        "ipa_dataset = load_and_preprocess_dataset(dataset_name, \"ipa\")\n",
        "ipa_model = fine_tune_model(ipa_dataset, \"ipa\", Wav2Vec2ForCTC, \"./results/ipa\", 10, 8, 5e-5)\n",
        "\n",
        "# 2. Prosody Analysis\n",
        "dataset_name = \"your_prosody_dataset\"\n",
        "prosody_dataset = load_and_preprocess_dataset(dataset_name, \"prosody\")\n",
        "prosody_model = fine_tune_model(prosody_dataset, \"prosody\", Wav2Vec2ForSequenceClassification, \"./results/prosody\", 10, 8, 5e-5)\n",
        "\n",
        "# 3. Non-Verbal Marker Annotation\n",
        "dataset_name = \"your_non_verbal_dataset\"\n",
        "non_verbal_dataset = load_and_preprocess_dataset(dataset_name, \"non_verbal\")\n",
        "non_verbal_model = fine_tune_model(non_verbal_dataset, \"non_verbal\", Wav2Vec2ForSequenceClassification, \"./results/non_verbal\", 10, 8, 5e-5)"
      ],
      "metadata": {
        "id": "bbJlaDW6WWA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}