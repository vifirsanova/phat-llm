{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPSzKeV7J/yzmhwfsjPbkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/phat-llm/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. **Download and Prepare the Data:**\n",
        "   - Transcribe a set of audio recordings with OpenAI Whisper\n",
        "   - IPA annotate audio files via GPT-4\n",
        "   - Use Praat and ELAN-annotated speech samples"
      ],
      "metadata": {
        "id": "7HOrVCErelPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install praatio\n",
        "!pip install pydub\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/vifirsanova/phat-llm.git\n",
        "%cd phat-llm"
      ],
      "metadata": {
        "id": "setup_environment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Load the Pre-trained Model:**"
      ],
      "metadata": {
        "id": "PMOFDqzzfP08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "\n",
        "model_name = \"openai/whisper-base\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "processor = WhisperProcessor.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "tJHWgeoefSyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Add LoRA Adapters:**"
      ],
      "metadata": {
        "id": "xrh9pOqefWKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "         r=16,  # Rank of the low-rank approximation\n",
        "         lora_alpha=32,  # Scaling factor\n",
        "         lora_dropout=0.1,  # Dropout probability\n",
        "         target_modules=[\"q_proj\", \"v_proj\"]  # Target modules to apply LoRA\n",
        "     )\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "i8rlInRbfdYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Prepare the Training Data:**"
      ],
      "metadata": {
        "id": "-PYitj30fd15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_inputs = processor(examples[\"audio\"], sampling_rate=16000, return_tensors=\"pt\")\n",
        "    with processor.as_target_processor():\n",
        "        labels = processor(examples[\"text\"], return_tensors=\"pt\").input_ids\n",
        "    return {\"input_features\": audio_inputs[\"input_features\"], \"labels\": labels}\n",
        "\n",
        "dataset = load_dataset('path/to/your/dataset')\n",
        "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "6VHpt3r6eajh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Train the Model:**"
      ],
      "metadata": {
        "id": "A-oTTTnUWfea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "         output_dir=\"./results\",\n",
        "         per_device_train_batch_size=16,\n",
        "         per_device_eval_batch_size=16,\n",
        "         num_train_epochs=3,\n",
        "         evaluation_strategy=\"epoch\",\n",
        "         logging_dir=\"./logs\",\n",
        "         logging_steps=10,\n",
        "         save_total_limit=2,\n",
        "         save_strategy=\"epoch\",\n",
        "         fp16=True,\n",
        "         learning_rate=5e-5,\n",
        "     )\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "         model=model,\n",
        "         args=training_args,\n",
        "         train_dataset=train_dataset,\n",
        "         eval_dataset=dataset[\"validation\"],\n",
        "         data_collator=processor,\n",
        "     )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZhDoJww7fwA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Evaluate the Model:**"
      ],
      "metadata": {
        "id": "j8-ELASKfwg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "5DcURa6Af2zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Inference:**"
      ],
      "metadata": {
        "id": "tQ94VPXNf4au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(audio_path):\n",
        "    audio_input = processor(audio_path, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    generated_ids = model.generate(input_ids=audio_input[\"input_features\"])\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    return transcription\n",
        "\n",
        "audio_path = '/path/to/your/audio/file.wav'\n",
        "transcription = transcribe_audio(audio_path)\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "inference_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Convert to XML through prompt-tuning**"
      ],
      "metadata": {
        "id": "kXJZ-E8efHYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_xml(transcription, output_path):\n",
        "    # Example function to convert transcription to XML\n",
        "    xml_content = f\"<transcription>{transcription}</transcription>\"\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(xml_content)\n",
        "\n",
        "output_path = '/path/to/your/output/file.xml'\n",
        "convert_to_xml(transcription, output_path)\n",
        "print(f'XML saved to {output_path}')"
      ],
      "metadata": {
        "id": "xml_conversion"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
