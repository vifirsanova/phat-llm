{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPSzKeV7J/yzmhwfsjPbkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/phat-llm/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. **Download and Prepare the Data:**\n",
        "   - transcribe a set of audio recordings with OpenAI Whisper\n",
        "   - IPA annotated audio files via GPT-4o\n",
        "   - Praat and ELAN-annotated speech samples"
      ],
      "metadata": {
        "id": "7HOrVCErelPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Load the Pre-trained Model:**"
      ],
      "metadata": {
        "id": "PMOFDqzzfP08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "\n",
        "model_name = \"openai/whisper-base\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "processor = WhisperProcessor.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "tJHWgeoefSyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Add LoRA Adapters:**"
      ],
      "metadata": {
        "id": "xrh9pOqefWKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "         r=16,  # Rank of the low-rank approximation\n",
        "         lora_alpha=32,  # Scaling factor\n",
        "         lora_dropout=0.1,  # Dropout probability\n",
        "         target_modules=[\"q_proj\", \"v_proj\"]  # Target modules to apply LoRA\n",
        "     )\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "i8rlInRbfdYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Prepare the Training Data:**"
      ],
      "metadata": {
        "id": "-PYitj30fd15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "  audio_inputs = processor(examples[\"audio\"], sampling_rate=16000, return_tensors=\"pt\")\n",
        "  with processor.as_target_processor():\n",
        "      labels = processor(examples[\"text\"], return_tensors=\"pt\").input_ids\n",
        "  return {\"input_features\": audio_inputs[\"input_features\"], \"labels\": labels}\n",
        "\n",
        "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "6VHpt3r6eajh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Train the Model:**"
      ],
      "metadata": {
        "id": "A-oTTTnUWfea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "         output_dir=\"./results\",\n",
        "         per_device_train_batch_size=16,\n",
        "         per_device_eval_batch_size=16,\n",
        "         num_train_epochs=3,\n",
        "         evaluation_strategy=\"epoch\",\n",
        "         logging_dir=\"./logs\",\n",
        "         logging_steps=10,\n",
        "         save_total_limit=2,\n",
        "         save_strategy=\"epoch\",\n",
        "         fp16=True,\n",
        "         learning_rate=5e-5,\n",
        "     )\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "         model=model,\n",
        "         args=training_args,\n",
        "         train_dataset=train_dataset,\n",
        "         eval_dataset=dataset[\"validation\"],\n",
        "         data_collator=processor,\n",
        "     )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZhDoJww7fwA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Evaluate the Model:**"
      ],
      "metadata": {
        "id": "j8-ELASKfwg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "eval_results"
      ],
      "metadata": {
        "id": "5DcURa6Af2zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Inference:**"
      ],
      "metadata": {
        "id": "tQ94VPXNf4au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Phonetic Transcription (IPA Symbols)\n",
        "dataset_name = \"your_ipa_transcription_dataset\"\n",
        "ipa_dataset = load_and_preprocess_dataset(dataset_name, \"ipa\")\n",
        "ipa_model = fine_tune_model(ipa_dataset, \"ipa\", Wav2Vec2ForCTC, \"./results/ipa\", 10, 8, 5e-5)\n",
        "\n",
        "# 2. Prosody Analysis\n",
        "dataset_name = \"your_prosody_dataset\"\n",
        "prosody_dataset = load_and_preprocess_dataset(dataset_name, \"prosody\")\n",
        "prosody_model = fine_tune_model(prosody_dataset, \"prosody\", Wav2Vec2ForSequenceClassification, \"./results/prosody\", 10, 8, 5e-5)\n",
        "\n",
        "# 3. Non-Verbal Marker Annotation\n",
        "dataset_name = \"your_non_verbal_dataset\"\n",
        "non_verbal_dataset = load_and_preprocess_dataset(dataset_name, \"non_verbal\")\n",
        "non_verbal_model = fine_tune_model(non_verbal_dataset, \"non_verbal\", Wav2Vec2ForSequenceClassification, \"./results/non_verbal\", 10, 8, 5e-5)"
      ],
      "metadata": {
        "id": "TqDK80B_V7m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to XML through prompt-tuning"
      ],
      "metadata": {
        "id": "kXJZ-E8efHYR"
      }
    }
  ]
}