{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPSzKeV7J/yzmhwfsjPbkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/phat-llm/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. **Download and Prepare the Data:**\n",
        "   - Transcribe a set of audio recordings with OpenAI Whisper\n",
        "   - IPA annotate audio files via GPT-4\n",
        "   - Use Praat and ELAN-annotated speech samples"
      ],
      "metadata": {
        "id": "7HOrVCErelPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install praatio\n",
        "!pip install pydub\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/vifirsanova/phat-llm.git\n",
        "%cd phat-llm"
      ],
      "metadata": {
        "id": "setup_environment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Load the Pre-trained Model:**"
      ],
      "metadata": {
        "id": "PMOFDqzzfP08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "\n",
        "model_name = \"openai/whisper-base\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "processor = WhisperProcessor.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "tJHWgeoefSyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Add LoRA Adapters:**"
      ],
      "metadata": {
        "id": "xrh9pOqefWKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "         r=16,  # Rank of the low-rank approximation\n",
        "         lora_alpha=32,  # Scaling factor\n",
        "         lora_dropout=0.1,  # Dropout probability\n",
        "         target_modules=[\"q_proj\", \"v_proj\"]  # Target modules to apply LoRA\n",
        "     )\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "i8rlInRbfdYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Prepare the Training Data:**"
      ],
      "metadata": {
        "id": "-PYitj30fd15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_inputs = processor(examples[\"audio\"], sampling_rate=16000, return_tensors=\"pt\")\n",
        "    with processor.as_target_processor():\n",
        "        labels = processor(examples[\"text\"], return_tensors=\"pt\").input_ids\n",
        "    return {\"input_features\": audio_inputs[\"input_features\"], \"labels\": labels}\n",
        "\n",
        "def load_and_preprocess_dataset(dataset_name, task):\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "    return train_dataset\n",
        "\n",
        "ipa_dataset = load_and_preprocess_dataset('ipa_transcription_dataset', 'ipa')\n",
        "prosody_dataset = load_and_preprocess_dataset('prosody_dataset', 'prosody')\n",
        "non_verbal_dataset = load_and_preprocess_dataset('non_verbal_dataset', 'non_verbal')"
      ],
      "metadata": {
        "id": "6VHpt3r6eajh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Fine-Tune the Model for Each Task:**"
      ],
      "metadata": {
        "id": "A-oTTTnUWfea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "def fine_tune_model(dataset, task, model_class, output_dir, num_train_epochs, per_device_train_batch_size, learning_rate):\n",
        "    model = model_class.from_pretrained(model_name)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    \n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,\n",
        "        save_strategy=\"epoch\",\n",
        "        fp16=True,\n",
        "        learning_rate=learning_rate,\n",
        "    )\n",
        "    \n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        eval_dataset=dataset[\"validation\"],\n",
        "        data_collator=processor,\n",
        "    )\n",
        "    \n",
        "    trainer.train()\n",
        "    return model\n",
        "\n",
        "ipa_model = fine_tune_model(ipa_dataset, 'ipa', WhisperForConditionalGeneration, './results/ipa', 3, 16, 5e-5)\n",
        "prosody_model = fine_tune_model(prosody_dataset, 'prosody', WhisperForConditionalGeneration, './results/prosody', 3, 16, 5e-5)\n",
        "non_verbal_model = fine_tune_model(non_verbal_dataset, 'non_verbal', WhisperForConditionalGeneration, './results/non_verbal', 3, 16, 5e-5)"
      ],
      "metadata": {
        "id": "ZhDoJww7fwA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Evaluate the Models:**"
      ],
      "metadata": {
        "id": "j8-ELASKfwg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset):\n",
        "    eval_results = model.evaluate()\n",
        "    print(eval_results)\n",
        "\n",
        "evaluate_model(ipa_model, ipa_dataset)\n",
        "evaluate_model(prosody_model, prosody_dataset)\n",
        "evaluate_model(non_verbal_model, non_verbal_dataset)"
      ],
      "metadata": {
        "id": "5DcURa6Af2zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Inference for Each Task:**"
      ],
      "metadata": {
        "id": "tQ94VPXNf4au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(model, audio_path):\n",
        "    audio_input = processor(audio_path, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    generated_ids = model.generate(input_ids=audio_input[\"input_features\"])\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    return transcription\n",
        "\n",
        "audio_path = '/path/to/your/audio/file.wav'\n",
        "ipa_transcription = transcribe_audio(ipa_model, audio_path)\n",
        "prosody_analysis = transcribe_audio(prosody_model, audio_path)\n",
        "non_verbal_annotation = transcribe_audio(non_verbal_model, audio_path)\n",
        "\n",
        "print(f'IPA Transcription: {ipa_transcription}')\n",
        "print(f'Prosody Analysis: {prosody_analysis}')\n",
        "print(f'Non-Verbal Annotation: {non_verbal_annotation}')"
      ],
      "metadata": {
        "id": "inference_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Convert to XML through Prompt-Tuning**"
      ],
      "metadata": {
        "id": "kXJZ-E8efHYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_xml(ipa_transcription, prosody_analysis, non_verbal_annotation, output_path):\n",
        "    xml_content = f\"\"\"\n",
        "    <transcription>\n",
        "        <ipa>{ipa_transcription}</ipa>\n",
        "        <prosody>{prosody_analysis}</prosody>\n",
        "        <non_verbal>{non_verbal_annotation}</non_verbal>\n",
        "    </transcription>\n",
        "    \"\"\"\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(xml_content)\n",
        "\n",
        "output_path = '/path/to/your/output/file.xml'\n",
        "convert_to_xml(ipa_transcription, prosody_analysis, non_verbal_annotation, output_path)\n",
        "print(f'XML saved to {output_path}')"
      ],
      "metadata": {
        "id": "xml_conversion"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
